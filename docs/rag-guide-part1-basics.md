# RAG建製指南（上集）：基礎概念與資料準備

## 🤖 什麼是RAG？

### RAG的本質：解決LLM的根本性資料問題

Retrieval-Augmented Generation（檢索增強生成），簡稱RAG，可以說是用來解決LLM三個痛點的。
（它的應用其實並不限於向量資料庫，但是因為LLM與向量資料庫天生合拍，所以RAG幾乎都是使用向量資料庫。很多人誤以為RAG=向量資料庫，或知識庫。這是完全不對的。）

**❌ 知識時效性問題** → **✅ 資料主權與時效性**

- **問題**：傳統LLM就像每年只做一次完整備份的唯讀資料倉儲，知識只更新到訓練截止日期，無法獲得最新資訊。
- **解決**：知識庫完全掌握在自己手中，可以隨時更新，確保AI回答的準確性和時效性。

**❌ 容易產生「幻覺」** → **✅ 可追溯性與可靠性**

- **問題**：LLM找不到答案時不會誠實說「我不知道」，而是基於相似資訊推斷，生成看似合理但實際錯誤的答案。
- **解決**：每個答案都有明確的資料來源，可直接追溯到原始文檔；通過提供具體參考資料，大幅減少憑空捏造答案的情況。

**❌ 缺乏領域專業知識** → **✅ 企業知識整合與成本效益**

- **問題**：LLM訓練於公開資料，無法獲得企業內部專有知識、標準作業程序、客戶資料等私有資訊
- **解決**：可整合任何企業內部資料；相比重新訓練模型，維護RAG系統成本更低且更新速度更快

### RAG如何解決這些問題？

RAG採用「先檢索，再生成」的兩階段策略，將LLM從一個封閉的知識系統轉變為一個可以即時查閱外部資料的智能助手。

**第一階段：建立知識索引**
這是一個類似資料庫ETL（Extract, Transform, Load）的過程：

- **抽取（Extract）- 資料整理**：收集企業內部各種文檔資料（PDF、Word、Wiki、資料庫記錄等），進行格式化處理，包括文檔解析、內容清理、編碼統一和質量檢查
- **轉換（Transform）- 分塊與嵌入**：將文檔切分成有意義的片段（chunking），並使用嵌入模型將每個片段轉換為數學向量（embedding），同時添加必要的元數據
- **載入（Load）- 資料庫存儲**：將文字片段和對應的向量存儲到向量資料庫中，涉及資料庫選擇（如Pinecone、Weaviate、PostgreSQL+pgvector）和索引結構設計

**第二階段：智能檢索與生成**
當用戶提問時：

1. 系統將用戶問題轉換為向量
2. 在向量資料庫中搜索最相關的知識片段
3. 將檢索到的資料作為上下文，與用戶問題一起送給LLM
4. LLM基於這些具體的參考資料生成準確答案

### RAG的挑戰與考量

**檢索品質決定一切**：如果檢索階段找到的資料品質不佳，LLM只會生成「精緻的垃圾」。文檔分塊策略、嵌入模型選擇、檢索演算法優化都是關鍵因素。

**系統複雜性增加**：引入向量資料庫意味著需要管理額外的基礎設施，包括備份、高可用性、監控等運維工作。

**資料管線維護**：當源文檔更新時，需要有穩定的機制來觸發重新處理和索引更新，這條資料管線的穩定性至關重要。

RAG本質上是一種聰明的架構模式，它將LLM從不可控的「黑盒子」轉變為以企業內部資料為核心的「可管理查詢處理引擎」。成功的關鍵在於將AI問題轉化為熟悉的資料管理問題：確保資料品質、優化檢索效能、維護穩定的資料管線。

## 📋 輸入資料格式整理

### 為什麼要分揀、格式化輸入資料及其重要性

在RAG系統中，輸入資料的格式化和預處理是決定最終效果的關鍵第一步。原始文檔往往包含多種格式和結構，如果直接餵給分塊和embedding過程，會導致語意理解偏差和檢索精度下降。

資料格式化的核心目標是**保留資訊的結構化語意**，讓Embedding模型能夠理解資料的內在邏輯關係，而不僅僅是文字的表面形式。這個步驟的品質直接影響：

- **檢索精確度**：格式化良好的資料能提供更準確的語意匹配
- **上下文完整性**：保留資料的原始結構關係，避免重要資訊碎片化
- **系統效能**：標準化的格式便於後續處理流程的自動化和優化

### 非文字資料格式處理策略

在拆分資料的時候，如果有表格，雖然表格可以以文字、csv、json、html table去表示。為什麼用html時取回的相似度以及範圍比較準確？

**核心原因**：HTML `<table>` 提供了最豐富的「結構化語意 (Structured Semantics)」。

讓我們用一個簡單的表格作為例子，來比較四種格式的差異。

**原始表格：**

| 姓名 | 部門   | 職稱       |
| :--- | :----- | :--------- |
| 張三 | 技術部 | 資深工程師 |
| 李四 | 市場部 | 專案經理   |

#### 1. 純文字 (Plain Text)

```
姓名  部門    職稱
張三  技術部  資深工程師
李四  市場部  專案經理
```

**問題分析**：

- **結構的遺失**：模型很難準確判斷「欄」的邊界，看到的是文字流而非二維網格
- **語意的模糊**：無法確定第一行是標頭，可能當作普通文字理解
- **結果**：生成的Embedding向量是模糊平均，資訊密度低

#### 2. CSV (Comma-Separated Values)

```
姓名,部門,職稱
張三,技術部,資深工程師
李四,市場部,專案經理
```

**問題分析**：

- **隱含的關係**：標頭和資料儲存格間的關係需要推斷，不夠可靠
- **缺乏全局視角**：缺少明確信號表示這是一個表格整體概念

#### 3. JSON (Array of Objects)

```json
[
  {
    "姓名": "張三",
    "部門": "技術部",
    "職稱": "資深工程師"
  },
  {
    "姓名": "李四",
    "部門": "市場部",
    "職稱": "專案經理"
  }
]
```

**問題分析**：

- **以「列」為中心**：弱化了表格的整體性和欄位間順序關係
- **冗餘資訊**：標頭在每筆資料中重複，可能引入雜訊

#### 4. HTML `<table>` 標籤 (最佳選擇)

```html
<table>
  <thead>
    <tr>
      <th>姓名</th>
      <th>部門</th>
      <th>職稱</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>張三</td>
      <td>技術部</td>
      <td>資深工程師</td>
    </tr>
    <tr>
      <td>李四</td>
      <td>市場部</td>
      <td>專案經理</td>
    </tr>
  </tbody>
</table>
```

**優勢分析**：

1. **明確的語意標記**：`<table>`、`<thead>`、`<tbody>`、`<th>`、`<td>`標籤明確區分結構層級
2. **保留二維關係**：巢狀結構完美保留表格的二維網格關係
3. **上下文完美封裝**：對Embedding模型來說是上下文完整、邊界清晰的完美資訊單元

**實務建議**：在處理表格資料時，盡可能將其轉換並保留為結構清晰的HTML `<table>` 格式，這是在Embedding階段提升檢索準確率最簡單也最有效的優化方法。

### 動態資料更新的處理策略

處理動態資料的關鍵，在於建立一個能回應**增、刪、改 (Create, Update, Delete)** 操作的自動化流程。

#### 偵測變更 (Change Data Capture, CDC)

這是整個流程的起點。你必須有一套機制能知道「來源資料變了」。常見的方法有：事件驅動 (Webhooks, Message Queues)、輪詢時間戳 (Polling with Timestamps) 或校驗和 (Checksums)。

#### 處理變更的 ETL 管線

**情境一：新增文件 (CREATE)**

1. 執行標準的分塊 (Chunking) 流程
2. 為每一個區塊，都附加上文件的元數據，尤其是 `document_id`
3. 將這些區塊進行嵌入 (Embedding)，並寫入向量資料庫

**情境二：修改文件 (UPDATE)**

1. **處理策略：採用「先刪後建 (Delete-then-Insert)」**。因為一個小改動可能影響所有分塊邊界，將一份文件視為一個原子單位來處理，是最穩健的策略
2. **執行步驟**：
   a. **刪除舊區塊**：根據 `document_id`，刪除所有與該文件相關的舊向量
   b. **建立新區塊**：將 **整份更新後的文件內容** ，重新進行一次完整的分塊和嵌入流程
   c. **寫入新區塊**：將新生成的所有向量和元數據，寫入向量資料庫

**情境三：刪除文件 (DELETE)**

1. 根據 `document_id`，直接向向量資料庫下達刪除指令

## 📄 文檔分塊策略：RAG系統的核心技術

### 什麼是文檔分塊？

想像你正在整理一座龐大的圖書館。每本書都是一份完整的文檔，但讀者往往只需要其中特定的章節或段落來回答問題。如果每次查詢都要翻閱整本書，不僅效率低下，還容易被無關內容干擾。

文檔分塊（Chunking）就像是將厚重的百科全書拆解成一張張專題卡片的過程。每張卡片包含一個完整的概念或知識點，既保持了內容的完整性，又方便快速檢索和理解。

這個「拆書成卡」的過程看似簡單，實際上是一門精細的藝術：

- **拆得太碎**：就像把一個完整的故事撕成單詞，失去了上下文，讀者無法理解完整含義
- **拆得太粗**：就像把整本字典當作一張卡片，雖然信息完整但查找困難，容易被雜訊淹沒
- **拆得恰當**：每張卡片都是一個自成體系的知識單元，既保持語意完整又便於精確檢索

### 為什麼分塊如此重要？

文檔分塊是RAG系統中最關鍵的技術環節，可以說是「地基工程」。它決定了後續檢索和生成的品質上限。分塊的核心挑戰是在保持語意完整性的同時，將大型文檔拆分成適合向量檢索的小片段。

**關鍵參數建議**：分塊大小通常200-1000字元（依文檔類型和精度需求調整），設置10-20%重疊確保資訊完整性，建立測試集持續驗證優化效果。記住：分塊是RAG地基，地基不穩，再強大的LLM也無法產生高品質回答。

### 📊 五種主流分塊策略核心差異：一張總表看懂

選擇合適的分塊策略是RAG系統成功的關鍵。以下對比表幫你快速掌握五種主流方法的特點，做出最佳選擇：

| 特性                   | 固定大小分塊         | 遞歸字元分塊           | 結構化分塊        | 語義分塊           | LLM輔助分塊        |
| ---------------------- | -------------------- | ---------------------- | ----------------- | ------------------ | ------------------ |
| **實現複雜度**   | 極簡                 | 簡單                   | 中等              | 複雜               | 最複雜             |
| **處理速度**     | 最快                 | 快                     | 中等              | 慢                 | 最慢               |
| **語意完整性**   | 最差⭐               | 良好⭐⭐⭐⭐⭐         | 優秀⭐⭐⭐⭐      | 最佳⭐⭐⭐⭐⭐     | 可控制⭐⭐⭐       |
| **檢索精確度**   | 低⭐                 | 中高⭐⭐⭐             | 高⭐⭐⭐⭐        | 極高⭐⭐⭐⭐⭐     | 極高⭐⭐⭐⭐⭐     |
| **資源消耗**     | 極低⭐⭐⭐⭐⭐       | 低⭐⭐⭐⭐             | 中等⭐⭐⭐        | 高⭐⭐             | 最高⭐             |
| **適用文檔**     | 通用                 | 通用                   | 結構化            | 高價值             | 少量核心           |
| **維護成本**     | 極低                 | 低                     | 中高              | 中高               | 高                 |
| **主要優點**     | 速度快、記憶體可預測 | 尊重語言邊界、平衡性佳 | 語意保真度最高    | 主題集中、檢索精準 | 智能重構、靈活性強 |
| **主要缺點**     | 破壞語意邊界         | 依賴文本格式規範       | 需專用解析器      | 計算成本高昂       | 成本極高、輸出不穩 |
| **適用場景**     | 快速驗證概念         | 一般文檔處理           | 技術文檔、API文檔 | 高價值知識庫       | 少量關鍵文檔       |
| **選擇時機**     | 🚫 避免生產使用      | ✅ 快速啟動首選        | 🎯 結構化文檔專用 | 💎 高端應用優化    | 🏆 精品內容打磨    |
| **混合策略建議** | 僅概念驗證           | 處理80%通用文檔        | 重要格式專用解析  | 核心內容精準優化   | 關鍵文檔創意重構   |

**🎯 最佳實踐建議**：從遞歸分塊開始（快速啟動），針對結構化文檔開發專用分塊器，對高價值內容採用語義分塊優化，最終實現混合策略部署以達到最佳效果。

#### 五大主流分塊策略詳解與實戰演練

##### 1. 固定大小分塊 (Fixed-size Chunking)

**原理**：設定固定的字元數（如512字元）和重疊範圍（如64字元），像切香腸一樣均勻分割文檔。

**核心特點**：實現最簡單但品質最差，像用尺子機械切割文本，完全不考慮語意邊界。雖然速度極快且記憶體可預測，但經常在關鍵資訊中間截斷，嚴重影響檢索效果。

**實戰演練示範**：
設定：`chunk_size = 70字元，chunk_overlap = 15字元`

```
測試文本：量子運算：加密學的雙面刃。量子運算利用量子位元(qubit)的疊加與糾纏特性...

結果：
Chunk 1: 量子運算：加密學的雙面刃。量子運算利用量子位元(qubit)的疊加與糾纏特性，能解決傳統電腦無法處理的複雜問題。其最著名的
Chunk 2: 的複雜問題。其最著名的應用之一是Shor演算法，它能有效破解當今廣泛使用的RSA和ECC加密體系，對金融與國安構成嚴
```

**問題分析**：語意在多處被截斷，如「其最著名的」、「構成嚴」，嚴重影響理解。

##### 2. 遞歸字元分塊 (Recursive Character Text Splitting)

**原理**：使用優先級分隔符號進行智能切分。首先嘗試段落分隔符（\n\n），不行則用句號（.），最後才用空格。

**核心特點**：智能切割的平衡選擇，優先按段落、句子等自然邊界分割。是目前主流框架的預設策略，在品質、效能和實現難度間取得最佳平衡，適合80%的通用場景。

**實戰演練示範**：
優先使用句號（。）作為分割邊界

```
結果：
Chunk 1: 量子運算：加密學的雙面刃。量子運算利用量子位元(qubit)的疊加與糾纏特性，能解決傳統電腦無法處理的複雜問題。
Chunk 2: 其最著名的應用之一是Shor演算法，它能有效破解當今廣泛使用的RSA和ECC加密體系，對金融與國安構成嚴重威脅。
Chunk 3: 然而，量子現象也催生了量子密鑰分發(QKD)，提供理論上無法破解的通信安全。
```

**優勢分析**：每個分塊都是完整句子，語意保持完整，這是實務中最常用的策略。

##### 3. 結構化分塊 (Document-specific/Structured Chunking)

**原理**：根據文檔的原生結構進行分塊，如Markdown的標題層級、HTML的標籤、PDF的章節等。

**核心特點**：針對特定格式量身定做的精準切割，像專業圖書管理員按章節目錄整理書籍。能完美保留文檔原生結構和層級關係，適合技術文檔、API手冊等結構化內容。

**實戰演練示範**：
保留文檔結構和元數據

```
結果：
Chunk 1:
  內容: 量子運算利用量子位元(qubit)的疊加與糾纏特性，能解決傳統電腦無法處理的複雜問題。其最著名的應用之一是Shor演算法...
  元數據: { "title": "量子運算：加密學的雙面刃", "section": "威脅分析" }
```

**特色分析**：保留了文檔結構資訊，便於實現基於標題或章節的篩選檢索。

##### 4. 語義分塊 (Semantic Chunking)

**原理**：使用嵌入模型計算相鄰句子間的語意相似度，在語意發生轉折的地方進行分割。

**核心特點**：AI驅動的智能語意分割，像有經驗的編輯識別文章的邏輯轉折點。通過計算句子間語意相似度找到自然分界線，確保每個分塊主題高度集中，檢索精確度極高。

**實戰演練示範**：
基於語意轉折點進行分割

```
結果：
Chunk 1 (威脅主題): 量子運算利用量子位元(qubit)的疊加與糾纏特性，能解決傳統電腦無法處理的複雜問題。其最著名的應用之一是Shor演算法，它能有效破解當今廣泛使用的RSA和ECC加密體系，對金融與國安構成嚴重威脅。
Chunk 2 (解決方案主題): 然而，量子現象也催生了量子密鑰分發(QKD)，提供理論上無法破解的通信安全。因此，各國正積極發展抗量子計算密碼學(PQC)，以應對即將到來的量子霸權時代。
```

**優勢分析**：在「然而」處識別出語意轉折，將威脅和解決方案分開，主題高度集中。

##### 5. LLM輔助分塊 (Agentic/LLM-based Chunking)

**原理**：直接使用大型語言模型來理解和重構文檔內容，如生成問答對、創建摘要等。

**核心特點**：最高智能的創造性重構，讓LLM像資深知識工程師一樣理解和改寫內容。能創造原文中不存在的結構（如Q&A對），靈活性最強但成本也最高，適合打造精品知識庫。

**實戰演練示範**：
重構為問答對格式

```
結果：
Chunk 1: Q: 量子運算為何對現有加密體系構成威脅？
        A: 因為其Shor演算法能有效破解當今廣泛使用的RSA和ECC加密體系，對金融與國家安全構成威脅。

Chunk 2: Q: 量子技術本身能提供何種資訊安全方案？
        A: 量子現象催生了量子密鑰分發(QKD)，它能提供理論上無法破解的通信安全。
```

**優勢分析**：創造了最佳的問答結構，用戶提問時直接命中相關QA的機率極高。

### 🔬 摘要增強分塊：進階優化策略

#### 什麼是摘要增強分塊？

摘要增強分塊（Summary-Enriched Chunking）是一種對基礎分塊方法的「二次加工」策略，就像資料倉儲中為原始交易資料建立彙總表一樣。它不取代前述的五種分塊方法，而是在其基礎上疊加一個語意增強層。

**核心工作流程**：

1. **基礎分塊**：使用遞歸字元分塊等方法將文件切成原始區塊
2. **LLM摘要增強**：為每個原始區塊生成簡潔的語意摘要
3. **分離存儲索引**：對摘要進行向量化建立索引，原始區塊作為關聯內容存儲

#### 與五種基礎方法的核心區別

**vs. 基礎分塊（固定/遞歸/結構化）**：基礎分塊負責「切割」，是物理層面操作；摘要增強負責「提煉」，是語意層面濃縮。

**vs. 語義分塊**：語義分塊決定「在哪裡切」（邊界問題），摘要增強決定「這塊講什麼」（內容問題）。

**vs. LLM輔助分塊**：LLM輔助會改變原始結構（如變成Q&A），摘要增強完整保留原始區塊，摘要只是附加的索引標籤。

#### 優缺點分析

**✅ 主要優點**：

- **大幅提升檢索精準度**：用戶查詢匹配簡潔摘要比匹配冗長原文更精確，解決查詢與文件間的語意鴻溝
- **保留完整上下文**：摘要僅用於檢索，答案生成時使用無損失的原始區塊
- **解決上下文歧義**：LLM生成的摘要為孤立區塊提供明確的上下文背景

**❌ 主要缺點**：

- **極高處理成本**：每個分塊都需要LLM API調用，百萬分塊意味著百萬次調用
- **系統複雜度增加**：資料結構從 `[chunk]`變成 `[(summary, original_chunk)]`
- **維護成本高昂**：文件更新時需要重新生成摘要，ETL管線變得昂貴緩慢

#### 實戰範例：零知識證明文檔

以「零知識證明」文檔為例，展示摘要增強的實際效果：

**原始分塊 1**：

```
原文：零知識證明（Zero-Knowledge Proof，簡稱ZKP）是一種密碼學協議，其中一方（證明者）能向另一方（驗證者）證明某個陳述為真，而無需透露除了「該陳述為真」這一事實之外的任何額外資訊。

LLM摘要：本區塊定義了零知識證明（ZKP）的基本概念，解釋它是一種允許證明者在不透露額外資訊的情況下證明陳述為真的密碼學協議。
```

**原始分塊 2**：

```
原文：ZKP的核心特性是完整性、可靠性和零知識性。

LLM摘要：本區塊闡述了零知識證明的三個核心特性：完整性、可靠性與零知識性。
```

**策略總結**：這是典型的「用空間和運算成本換取查詢精準度」的策略。適合對問答品質要求極高且知識庫相對穩定的應用場景，但對頻繁更新、預算有限的系統來說成本可能過高。

### 🗃️ Metadata優化分塊：結構化檢索增強

#### 什麼是Metadata優化分塊？

Metadata優化分塊是將原本只有 `TEXT`欄位的非結構化資料，升級成擁有完整欄位的結構化資料表。就像資料庫中使用 `WHERE`語句進行精確過濾一樣，它讓我們能在進行昂貴的向量搜索前，先用精確條件縮小範圍。

**核心調取流程**：

1. **元數據過濾**：使用 `WHERE`條件快速篩選符合條件的分塊
2. **語意搜索**：在已過濾結果中進行向量相似度搜索
3. **結果回傳**：獲得既符合條件又語意相關的精準結果

**使用範例**：當用戶提問「Dr. Reed在2024年後發表的區塊鏈相關技術有哪些？」時，系統會先執行 `SELECT * WHERE author = 'Dr. Evelyn Reed' AND publication_date > '2024-01-01' AND tags CONTAINS 'Blockchain'`，將百萬筆資料縮小到幾百筆，再進行語意搜索。

**重要性評估**：對於任何想要在生產環境中部署的RAG應用，使用元數據進行過濾不是選項，而是必需品。它能帶來壓倒性的效能與精度優勢，與「摘要增強」結合更是能將檢索品質推向極致的王道組合。

#### 與摘要增強的差別對比與優缺點

增加元數據和「摘要增強」一樣，都不是獨立的「分塊方法」，而是對基礎分塊結果的增強手段。

**核心差別對比表**：

| 特性               | **Metadata增強**                                          | **摘要增強**                 |
| ------------------ | --------------------------------------------------------------- | ---------------------------------- |
| **資料類型** | 結構化鍵值對，如 `{"category": "Cryptography", "year": 2024}` | 非結構化自然語言文字，由LLM生成    |
| **主要目的** | 前置過濾，在語意搜索前快速縮小搜尋範圍                          | 語意提煉，優化向量匹配精準度       |
| **查詢方式** | 精確匹配/範圍查詢，如 `category = 'Cryptography'`             | 相似度搜索，比較查詢向量與摘要向量 |
| **作用階段** | 前置過濾階段                                                    | 語意匹配階段                       |

**✅ Metadata增強優點**：

- **大幅提升檢索效率與降低成本**：先用WHERE過濾將百萬級資料縮小到幾百筆，速度提升數個數量級
- **實現精準可控查詢**：支持「搜尋關於ZKP且發表於2024年後的安全類文件」等確定性過濾
- **賦能複雜應用邏輯**：可建構電商級別的多維度篩選器介面
- **易於整合**：主流向量資料庫（Pinecone、Weaviate、Chroma）都原生支援

**❌ Metadata增強缺點**：

- **強烈依賴元數據品質**：原始文件缺乏乾淨可用的元數據時方法失效
- **增加資料導入複雜性**：ETL流程需準確解析並關聯元數據與文本區塊

**結論**：兩者是黃金拍檔而非競爭關係。最先進的RAG檢索流程是「先用元數據過濾，再對結果進行語意搜索」。

#### 建議的Metadata項目與其目的

Metadata設計需要服務於三個核心目的：**精準過濾**（在向量搜索前用WHERE條件縮小範圍）、**高效回溯**（快速了解向量的身世和上下文）、**豐富上下文**（為重排序和答案生成提供判斷依據）。以下是適用於大多數企業級RAG系統的完整藍圖：

##### 🔍 第一類：來源追溯類 - 「這個Chunk是誰？」

*最基礎、最重要的資訊，是所有後續操作的根本*

- **`document_id`** (字串)：原始文件的唯一識別碼，是最重要的欄位。所有文件的更新、刪除、關聯操作都依賴這個ID，就像資料表的主鍵
- **`chunk_id`** (字串)：單一chunk的唯一識別碼，通常設計為 `f"{document_id}_chunk_{N}"`，有助於日誌記錄和問題排查
- **`source_uri`** (字串)：原始文件的可訪問路徑，用於除錯或向用戶展示原文連結

##### 📍 第二類：內容結構類 - 「這個Chunk在哪裡？」

*對於實現上下文擴展重排序至關重要*

- **`page_number`** (整數)：標示chunk來自文件的哪一頁，是最直觀的回溯方式
- **`section_header`** (字串)：chunk所屬的章節標題，提供強大的局部上下文
- **`position_in_page`** (整數)：chunk在頁面中的位置序號，實現滑動窗口上下文的必備欄位
- **`chunk_type`** (字串)：內容類型標識，如 `text`、`table`、`title`、`list_item`，用於細緻的Reranking處理

##### ⏰ 第三類：時間維度類 - 「這個Chunk何時的？」

*對於資料過濾和維護至關重要*

- **`last_modified_at`** (時間戳)：原始文件的最後修改時間，實現增量更新的核心欄位
- **`indexed_at`** (時間戳)：chunk被索引和嵌入的時間，有助於追蹤資料新鮮度

##### 🔒 第四類：權限與歸屬類 - 「誰能看這個Chunk？」

*企業環境中安全性的基石，絕對不能省略*

- **`access_level`** (字串/陣列)：存取權限等級定義，RAG系統必須用當前用戶權限過濾此欄位
- **`department`** (字串)：資料歸屬部門，便於進行部門級知識篩選

##### 🎯 第五類：語意增強類 - 「這個Chunk講了什麼？」

*選擇性欄位，但能極大增強檢索效果，避免語意稀釋*

- **`summary`** (字串)：LLM生成的簡短摘要，可選擇對摘要而非原文進行Embedding
- **`keywords`** (字串陣列)：LLM提取的核心關鍵字，用於BM25混合式搜尋
- **`questions`** (字串陣列)：假設性文件嵌入(HyDE)技巧，預先生成chunk可能回答的問題

**實施建議**：

1. **分階段實施**：第一、二、四類是基礎必需，第三類對維護重要，第五類是進階優化
2. **一致性是王道**：確保ETL管線為所有文件產生結構一致、內容可靠的metadata

好的metadata設計就是RAG系統的骨架，支撐起更複雜、更精準、更安全的上層應用。

#### 實戰範例：零知識證明文檔

以「零知識證明」文檔為例，展示Metadata優化分塊的完整實施過程：

##### 原始文檔Metadata

```json
{
  "document_id": "ZKP_Intro_Whitepaper_v2",
  "author": "Dr. Evelyn Reed",
  "publication_date": "2024-10-26",
  "category": "Cryptography",
  "tags": ["ZKP", "Blockchain", "Security"],
  "security_level": "Public"
}
```

##### 分塊結果示例

**分塊1**：

```
內容：零知識證明（Zero-Knowledge Proof，簡稱ZKP）是一種密碼學協議，其中一方（證明者）能向另一方（驗證者）證明某個陳述為真，而無需透露除了「該陳述為真」這一事實之外的任何額外資訊。

基礎Metadata：
{
  "document_id": "ZKP_Intro_Whitepaper_v2",
  "chunk_number": 1,
  "author": "Dr. Evelyn Reed",
  "publication_date": "2024-10-26",
  "category": "Cryptography",
  "tags": ["ZKP", "Blockchain", "Security"]
}
```

**分塊2**：

```
內容：ZKP的核心特性是完整性、可靠性和零知識性。

基礎Metadata：
{
  "document_id": "ZKP_Intro_Whitepaper_v2",
  "chunk_number": 2,
  "author": "Dr. Evelyn Reed",
  "publication_date": "2024-10-26",
  "category": "Cryptography",
  "tags": ["ZKP", "Blockchain", "Security"]
}
```

##### 完整Metadata範例

包含所有五類metadata的完整示例：

```json
{
  "document_id": "ZKP_Intro_Whitepaper_v2",
  "chunk_id": "ZKP_Intro_Whitepaper_v2_chunk_2",
  "source_uri": "https://company.com/whitepapers/zkp_intro.pdf",
  "page_number": 3,
  "section_header": "1.2 Core Properties",
  "position_in_page": 0,
  "chunk_type": "text",
  "last_modified_at": "2025-06-15T10:30:00Z",
  "indexed_at": "2025-07-06T12:00:00Z",
  "access_level": ["public"],
  "department": "Research",
  "summary": "This chunk outlines the three fundamental properties of Zero-Knowledge Proofs: completeness, soundness, and zero-knowledge.",
  "keywords": ["ZKP", "completeness", "soundness", "zero-knowledge"]
}
```

這個範例展示了如何將原始文檔metadata繼承到每個分塊，並添加chunk特有的結構化資訊，為精準檢索提供完整的資料基礎。

### 分塊框架支援

#### 主流框架介紹

在RAG實作中，最主流的Python框架是**LangChain**和**LlamaIndex**。這兩個框架都是**RAG應用的編排和管理工具**，而不是只在chunking或embedding的某個環節使用。它們是**RAG流程的統籌者**，管理從文檔處理、分塊、embedding、檢索到生成的整個pipeline，提供各種現成的組件庫，並簡化不同工具間的整合。

#### 各自特色與建議

**LangChain 如同一個「瑞士軍刀」**：它什麼都有，工具箱非常龐大，極其靈活。你可以用它的基礎元件（如 `RecursiveCharacterTextSplitter`）快速開始，也可以用它的 `LLMChain`模組去搭建非常複雜的Agentic或Semantic分塊流程。缺點是學習曲線較陡峭，有時候會感覺過於「模板化」。

**LlamaIndex 更像一個「專為RAG優化的索引引擎」**：它從一開始就聚焦在「將外部資料接入LLM」這個核心問題上，因此在資料解析(`Reader`)、索引(`Index`)和分塊(`NodeParser`)這些環節上，做得非常深入和專精。它原生支援「語義分塊」就是最好的證明。如果你的專案核心就是RAG，LlamaIndex往往是更直接、更專注的選擇。

**實用建議**：

* **如果你的專案剛起步，資料主要是非結構化的純文字**：直接用**LangChain**或**LlamaIndex**的**`RecursiveCharacterTextSplitter`/`SentenceSplitter`**開始，這是最穩妥的基準線。
* **如果你的資料來源品質很高，是Markdown、程式碼或Confluence**：花點時間研究框架中對應的**結構化解析器(Structured Parser/Splitter)**，這個投入絕對值得。
* **如果你的應用對「問答精準度」的要求達到了極致，不惜代價**：可以優先考慮**LlamaIndex的 `SemanticSplitter`**，或者在LangChain中自己動手實現一個類似的流程。
* **至於LLM輔助分塊和摘要增強**：將它們視為進階的「**優化模組**」。在你建立了第一版RAG系統後，如果發現檢索效果不佳，再考慮引入這些高成本的方案來對特定的、重要的資料進行二次加工。

#### 框架支援情況總覽

| 分塊策略 (Chunking Strategy)           | 主要框架支援情況                                                                                                                                                                                                                                                                                                    | DBA 的備註 (My Notes)                                                                                                                                                                             |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. 固定大小分塊 (Fixed-size)** | **全面內建支援 (Commodity Feature)** `<br>`-**LangChain:** `FixedSizeTextSplitter<br>`-**LlamaIndex:** `SentenceSplitter`可配置成類似行為 `<br>`- 其他框架也都有類似基礎功能                                                                                                              | 這是最基礎的「Hello World」級別功能。快速驗證想法時可用，但**生產環境中應避免** ，因為它太容易破壞語意。                                                                                    |
| **2. 遞歸字元分塊 (Recursive)**  | **主流框架的預設選項 (The De Facto Standard)** `<br>`-**LangChain:** `RecursiveCharacterTextSplitter`是官方最推薦、也最常用的選項。`<br>`-**LlamaIndex:** `SentenceSplitter`(其預設行為) 與此非常相似，會優先按句子、片語等邊界切分。                                                     | **這是你的主力戰馬 (Your Workhorse)** 。在 80% 的通用場景下，這都是最合理、最平衡的選擇。它在效能和品質之間取得了最佳的平衡點。                                                             |
| **3. 按結構分塊 (Structured)**   | **廣泛內建支援 (Widely Supported)** `<br>`-**LangChain:**提供多種專用 `splitter`，如 `MarkdownTextSplitter`,`PythonCodeTextSplitter`等。`<br>`-**LlamaIndex:**這是它的強項，其 `NodeParser`和 `Reader`生態系對多種格式（Markdown, PDF, JSON, Confluence）有非常好的結構化解析支援。             | 如果你的資料源是結構化的（API 文件、程式碼、內部 Wiki），**務必使用這個** 。這就像是針對特定資料庫使用專用的 JDBC/ODBC 驅動程式，效能和可靠性是最高的。                                     |
| **4. 語義分塊 (Semantic)**       | **部分框架原生支援 (Natively Supported by Some)** `<br>`-**LangChain:** **沒有**直接內建這個 `splitter`。但你可以利用其工具自行 **客製化實現** 。`<br>`-**LlamaIndex:**  **有內建支援** 。它提供一個名為 `SemanticSplitterNodeParser`的元件，可以直接使用。             | **這是 LlamaIndex 的一個亮點和優勢** 。如果你對檢索品質有極致的追求，且願意付出較高的運算成本，LlamaIndex 在這方面提供了開箱即用的解決方案。                                                |
| **5. LLM 輔助分塊 (Agentic)**    | **所有框架的核心功能 (A Core Capability of All Frameworks)** `<br>`-**LangChain:**這本身就是其「鏈 (Chains)」和「代理 (Agents)」的核心應用。你需要自行**建構一個處理鏈**(`LLMChain`) 來實現。`<br>`-**LlamaIndex:**同樣是其核心功能，可以透過其 Ingestion Pipeline 結合 `LLM`元件來實現。 | 這已經不單純是「分塊」，而是「**資料轉換與生成 (Data Transformation & Generation)** 」的範疇了。所有 LLM 編排框架都能做到，但需要你自己定義工作流程。它非常強大，但成本和複雜度也是最高的。 |

## 🔢 Embedding 指南

### **第一部分：什麼是 Embedding？**

這背後的魔法，來自於**高維向量空間 (High-dimensional Vector Space)** 的概念。

1. **轉換為座標** ：一個 Embedding 模型，就是一個巨大的神經網路。它的工作，是把任何一段文字（一個詞、一個句子、一個區塊）讀進去，然後輸出一個由幾百到幾千個數字組成的列表，我們稱之為「 **向量 (Vector)** 」。例如 `[0.012, -0.45, 0.88, ..., -0.15]`。你可以把這個向量想像成是這段文字在一個超高維度空間中的「 **語意座標** 」。
2. **從海量資料中學習「位置關係」** ：模型在數十億頁的文本上進行訓練。在訓練過程中，它學會了一個規則：**如果兩個詞或兩段話，在各種文章中出現的上下文 (context) 越相似，那它們的「語意座標」就應該越接近。**
3. **用「距離」度量「相似度」** ：當我們有了兩個文字的向量座標後，要判斷它們有多相似，就變成了一個數學問題。我們不再是比較字串，而是計算這兩個向量在空間中的「 **距離** 」或「 **夾角** 」。最常用的方法是「 **餘弦相似度 (Cosine Similarity)** 」。你可以簡單理解為：

* 如果兩個向量指向 **完全相同的方向** （夾角為0°），代表它們語意極度相似，相似度為 1。
* 如果兩個向量 **完全正交** （夾角為90°），代表它們語意無關，相似度為 0。
* 如果兩個向量指向 **完全相反的方向** （夾角為180°），代表它們語意完全相反，相似度為 -1。

### **第二部分：主流模型與框架對比**

選擇 Embedding 模型，就像我們以前在選擇資料庫引擎（InnoDB vs. MyISAM）或作業系統（Linux vs. Windows）一樣，需要在**效能、成本、控制權**之間做權衡。

#### **模型對比**

| 陣營                   | 代表模型與說明                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 優點                                         |
| :--------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------- |
| **商業閉源模型** | **OpenAI:** `text-embedding-3-large` / `text-embedding-3-small`: 這是目前的業界標竿，通用性極強，效果穩定。`large` 更精確，`small` 更具成本效益。`<br><br>`**Cohere:** `embed-v3.0`: Cohere 的模型在某些企業級檢索任務上表現非常出色，特別強調其在真實世界 RAG 場景的優化。`<br><br>`**Google (Vertex AI):** `text-embedding-004` (代號 Gecko): Google 的最新模型，整合在 GCP 生態系中，效能強勁。                                                                                                                                     | 方便、強大，不用自己煩惱維運。               |
| **開源模型**     | **NVIDIA:** `NV-Embed-v2` - 目前在許多榜單上都名列前茅，展現了強大的通用能力。`<br><br>`**BAAI:** `bge-m3` - BGE 系列模型，特別是 `m3` 版本，支援超過 100 種語言，是處理多語言（包含繁體中文）內容的絕佳選擇。`<br><br>`**Alibaba:** `gte-Qwen2-7B-instruct` - 阿里巴巴的通義千問系列衍生的嵌入模型，在中文和英文上都有頂尖的表現。`<br><br>`**Nomic:** `nomic-embed-text-v1.5` - 一個非常受歡迎且高效的模型。`<br><br>`你可以在 [Hugging Face 的 MTEB 排行榜](https://huggingface.co/spaces/mteb/leaderboard) 上看到最新的戰況。 | 資料隱私性高、長期成本可能更低、控制權最大。 |

#### **框架對比**

| 框架                             | 主要用途                     | 特點                                                                                                                              |
| :------------------------------- | :--------------------------- | :-------------------------------------------------------------------------------------------------------------------------------- |
| **LangChain / LlamaIndex** | **高層次應用框架**     | 內建了與上述所有模型的連接器 (Connectors)，可以讓你用幾行程式碼就輕鬆切換不同的 Embedding 供應商，專注於搭建完整的 RAG 應用流程。 |
| **Sentence-Transformers**  | **核心 Python 函式庫** | 使用開源模型時最核心的函式庫，提供了載入、訓練和使用各種嵌入模型的標準化介面，更專注於 Embedding 模型本身的操作。                 |

### **第三部分：如何定位向量**

這正是 Embedding 模型強大的地方，它依靠**上下文**來解決這些問題。

| 語言現象                       | 範例                                                             | Embedding 模型的處理方式                                                                                                                                                                                                                                                      |
| :----------------------------- | :--------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **同義詞 (Synonyms)**    | `公司`vs.`企業`                                              | 由於「公司」和「企業」在訓練資料中總是出現在相似的上下文裡（例如，後面都接著「財報」、「產品」、「執行長」），模型會自動學習到它們的語意是相近的，因此會將它們的向量放在空間中非常接近的位置。                                                                                |
| **代名詞 (Pronouns)**    | "ZKP 是一種協議。**它**能實現..."                          | 模型在處理第二句話時，其內部的「注意力機制 (Attention Mechanism)」會將「**它** 」這個詞的注意力權重，大量分配給前文的「 **ZKP** 」。因此，第二句話的整體向量會因為這個上下文關聯，而與「ZKP」的向量非常接近。模型理解的是 **整句話的意義** ，而非孤立的詞。 |
| **多義詞 (Polysemy)**    | 1. 我買了**蘋果**手機。`<br>`2. 我吃了 **蘋果** 。 | **上下文是關鍵** 。在第一句中，「手機」、「買了」這些詞會將「蘋果」的向量推向「科技公司」的語意區域。在第二句中，「吃了」這個詞會將「蘋果」的向量推向「水果」的語意區域。因此，這兩句話中「蘋果」的有效語意完全不同，它們的向量座標在空間中也會相距甚遠。               |
| **簡稱 (Abbreviations)** | `零知識證明`vs.`ZKP`                                         | 模型在訓練時看過無數次 `Zero-Knowledge Proof (ZKP)`或 `零知識證明 (ZKP)`這樣的用法，也看過它們在文章中被交替使用。因此，模型學會了 `ZKP`和它的全稱是指向同一個概念的「指標」，並將它們的向量映射到幾乎相同的位置。                                                      |

### **第四部分：微調模型 (Fine-tuning)**

在考慮微調 Embedding 模型之前，請先確定你已經將 RAG 的其他環節（如分塊策略、摘要增強、元數據過濾）優化到極致。微調是個高成本、高技術門檻的選項。很多時候，更好的 RAG 流程設計，就能解決 80% 的問題。把它當作你最後的、也是最強大的王牌。

#### **第一步：準備「教材」(Data Preparation)**

微調的成敗，90% 取決於你的「教材」品質，也就是你的訓練資料集。你需要創建一個高品質的「 **知識對（Knowledge Pairs）** 」或「 **知識三元組（Knowledge Triplets）** 」。

* **成對資料 (Paired Data):** `(查詢, 正相關段落)`
* **三元組資料 (Triplet Data):** `(錨點/查詢, 正相關段落, 負相關段落)`

你需要準備成千上萬對這樣的高品質資料。資料品質越高，微調效果越好。這一步是整個環節中最耗時、也最有價值的。

#### **第二步：選擇微調方法**

| 微調方法                              | 說明                                                                                                                                                                                                                                                                           | 優點                                                                                                                                                                                                                                                                                              | 缺點                                                                                                                                                                                                                                                                          |
| :------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **完全微調 (Full Fine-tuning)** | 把整個 Embedding 模型的所有參數（權重），都用你的資料集進行重新訓練。這就像是為了你的特殊需求，把整個 PostgreSQL 資料庫的原始碼下載下來，修改核心演算法後，**重新編譯 (Recompile) 整個資料庫引擎** 。                                                                    | 理論上能達到最佳的適應效果，模型與你的領域知識融合得最徹底。                                                                                                                                                                                                                                      | **成本極高** ：需要非常強大的 GPU 叢集（例如 A100 / H100），訓練時間長，費用昂貴。`<br><br>`**風險較高** ：如果你的資料集不夠大或不夠好，很容易導致「 **災難性遺忘 (Catastrophic Forgetting)** 」，也就是模型學會了你的黑話，卻忘了「貓是動物」這種常識。 |
| **參數高效微調 (PEFT / LoRA)**  | **凍結 (freeze) 住原始模型的絕大部分參數（99%以上），只訓練一小部分新增的、或是特定的參數** 。這不像重新編譯資料庫，而更像是為資料庫引擎 **安裝一個高度優化的「外掛 (Plugin)」或「擴充套件 (Extension)」** 。最主流的技術是 **LoRA (Low-Rank Adaptation)**。 | **成本極低** ：對 GPU 的要求大幅降低，甚至在消費級顯卡（如 RTX 4090）上就能進行。`<br><br>`**部署靈活** ：你可以用一個基礎模型，搭配多個不同的 LoRA 適配器，來服務於不同的行業或任務。`<br><br>`**有效防止遺忘** ：因為沒有動原始模型的主體，它能很好地保留原有的通用知識。 | 理論效果可能略遜於完美的完全微調，但成本效益極高。                                                                                                                                                                                                                            |

#### **微調實戰路線圖**

1. **步驟 0：竭盡全力優化 RAG** 。這是前提。
2. **步驟 1：建立高品質的「知識對/三元組」資料集** 。投入 80% 的精力在這裡。
3. **步驟 2：選擇一個強大的開源模型作為基礎** 。例如 BAAI 的 `bge-m3` 或 Alibaba 的 `gte-Qwen2` 系列。
4. **步驟 3：採用 PEFT (LoRA) 方法進行微調** 。這是目前成本效益最高的選擇。
5. **步驟 4：建立一個「評估集」** ，用來測試微調前後的模型，用數據來驗證你的工作成果。
6. **步驟 5：評估通過後，將你訓練好的 LoRA 適配器與基礎模型結合，部署到你的 RAG 系統中**。

### **第五部分：Embedding 上線前檢查清單**

1. **成本的精確估算 (Precise Cost Estimation)** ：Embedding 不是免費的午餐。它的成本分為兩部分，你必須都考慮到：
   * **初始索引成本 (Initial Indexing Cost)** ：當你將所有歷史文件轉換為向量時，會產生一筆**一次性但巨大**的開銷。
   * **持續查詢與更新成本 (Ongoing Query & Update Cost)** ：每次使用者查詢、每份新文件更新，都會產生費用。
2. **效能與延遲的預期 (Performance & Latency Expectations)** ：明確你的應用場景對延遲的要求。如果是自行部署，需要考慮 GPU 的批次處理能力，以應對高併發場景。
3. **版本控制與模型升級策略 (Versioning & Model Upgrade Strategy)** ：這是最多人忽略，但事後最痛苦的一點。不同模型的向量不互相兼容。當你決定從 `v1` 升級到 `v2` 時，你必須 **重新索引你所有的文件** 。
   * **我的建議** ：務必儲存原始文本！並在你的向量資料庫中，為每一個向量 **增加一個 `model_version` 的元數據欄位** 。
4. **向量維度 (Vector Dimensionality) 的權衡** ：
   * **高維度** ：通常包含更豐富的語意資訊，可能更精確。但它佔用更多的儲存空間和記憶體，向量計算和搜索的速度也更慢。
   * **低維度** ：佔用資源少，搜索速度快，但可能會損失一些語意的細膩度。
5. **資料隱私與安全的合規性 (Data Privacy & Security Compliance)** ：
   * **使用商業 API** ：你的原始文本資料會被傳送到 OpenAI 或 Google 的伺服器上進行處理。這對於金融、醫療或涉及國家機密的行業來說，可能是 **完全不可接受的** 。
   * **使用開源模型** ：所有資料都在你自己的內網或私有雲環境中處理，擁有最高的資料主權和安全性。
6. **評估，評估，再評估 (Evaluate, Evaluate, Evaluate)** ：在投入巨大成本進行全面索引之前，你必須先驗證模型的有效性。建立一個「黃金標準評估集」，用小規模實驗測試 2-3 個候選的 Embedding 模型。用數據說話，在你自己的資料集上跑出來的結果，才是唯一的事實。

### **第六部分：處理動態資料更新**

這個問題是將 RAG 系統從「靜態的知識庫」推向「 **動態、有生命的情報中心** 」的核心。你需要將你的向量索引庫，視為你主要資料來源的一個「 **唯讀複本 (Read Replica)** 」或「 **物化視圖 (Materialized View)** 」。

#### **核心設計原則**

1. **唯一且穩定的 ID 是你的生命線** ：每一份原始文件都**必須**有一個全系統唯一且不會改變的 `document_id`。
2. **增量更新，而非全量刷新 (Incremental Updates > Full Refresh)** ：絕對不要因為少量數據變動，就刪除整個向量資料庫再重新建立。
3. **非同步處理 (Asynchronous Processing)** ：資料的同步、分塊和嵌入，應該在一個背景任務中進行，完全與使用者查詢的流程分開。

### **第七部分：總結**

Embedding 模型的強大之處，在於它不是在做死板的關鍵字比對，而是在一個被海量知識預先「整理」過的語意空間中，進行概念層次的「遠近」判斷。選擇哪種模型，取決於你的 **預算（商業 vs. 開源）、資料語言（中文支援度）和對精準度的要求** ，這是一個典型的系統設計權衡。

將 Embedding 這一步，視為一個需要長期維護和迭代的 **核心數據基礎設施 (Core Data Infrastructure)** ，而不僅僅是一個一次性的腳本任務。提前思考清楚**成本、效能、升級、安全和評估**這五個方面，是區分一個能穩定運行的生產級系統和一個曇花一現的原型系統的關鍵。微調是一個威力強大的工具，但它考驗的不是你的演算法知識，而是你對自身業務領域的理解深度，以及你準備高品質資料的耐心和紀律。這和我們管理資料庫的哲學是完全一致的：**一切的根基，都在於資料本身。**

---

## 📚 系列導航

本篇為RAG建製指南上集，涵蓋了RAG的基礎概念、文檔分塊策略、摘要增強和metadata優化等核心技術。

- [中集：檢索與生成](./rag-guide-part2-retrieval.md) - 深入了解向量檢索、生成控制和效能優化
- [下集：進階應用](./rag-guide-part3-advanced.md) - 企業級部署、安全性和最佳實踐
- [終集：評估效能、自動學習與系統升級](./rag-guide-part4-optimization.md) - 效能評估框架、自動化監控、自學習機制、持續優化實踐

---
