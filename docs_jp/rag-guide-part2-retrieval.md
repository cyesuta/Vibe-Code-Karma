# RAG構築ガイド（中）：検索と生成

## 🔍 検索方式

### 検索の二大流派：キーワード vs. セマンティック

情報検索の分野には、互いに補完し合う二つの大きな流派が存在する。**キーワード検索**（例：BM25、TF-IDF）は厳密な単語の一致に焦点を当て、統計的手法でスコアを計算し、`PostgreSQL v15.3`のような特定のキーワードに正確にヒットすることを得意とするが、意味を理解できず、「車」と「自動車」を無関係な単語と見なす。一方、**ベクトル検索**（例：FAISS、HNSW、IVFFlat）はテキストの意味を理解し、クエリと文書をベクトルに変換して意味空間で比較するため、同義語の問題を扱えるが、製品型番`NVDA-H100`のような具体的で珍しいキーワードには十分に敏感ではない。両者は完璧な補完関係を形成している。

これらの長所と短所がちょうど補完的であるため、我々は後に**ハイブリッド検索（Hybrid Search）**という、現在最も先進的な戦略の一つを使用する。これは、ベクトル検索であなたの質問の「意味」を理解し、**同時に**BM25で質問の中の「キーワード」を捉え、最終的に両者の結果を融合させることで、あなたの意図を理解しつつ、いかなる重要な詳細も見逃さないことを保証するものである。

### ベクトル検索の二つの戦略

ベクトル検索には二つの基本的な戦略がある。**厳密検索**はクエリベクトルと各ベクトルの真の距離を計算し、100%の正確性を保証するが非常に遅い（O(n)の計算量）。**ANN検索**はアルゴリズムを用いて最も類似した結果を近似的に見つけ出し、わずかな正確性を犠牲にして大幅な速度向上を実現する。大規模データを扱えるが、最適解を逃す可能性がある。実際の応用では、**データ分類戦略＋ANN検索**が黄金の組み合わせである。検索範囲を絞り込み、意味的なノイズを排除することで、純粋な環境下でのANNの正確性を向上させ、RAGシステムの回答をより正確で信頼性の高いものにする。

### ベクトルインデックス手法の選択戦略

**IndexFlatIP（Flat）**は100%の正確性を提供するがクエリが非常に遅く、小規模データセット（<5万件）やベンチマークテストに適している。**IVFFlat**はパーティション戦略を採用してメモリと速度のバランスを取り、メモリが限られた環境や数百万規模のデータに適している。**HNSW**は速度と正確性の総合的なパフォーマンスが最も優れたインデックスであり、リアルタイムシステムに適しているがメモリ消費が大きい。**FAISS**は柔軟な検索ライブラリで、多様なインデックスとGPUアクセラレーションをサポートし、超大規模なデプロイに適している。

選択の原則：正確性を追求するならFlat、メモリに制約があるならIVFFlat、極速が必要ならHNSW、柔軟性が必要ならFAISSを選ぶ。

### 6つの検索手法の包括的比較

| 検索手法（タイプ） | 中核原理と応用シーン | 主な長所と短所 | 実装難易度とコスト |
|---|---|---|---|
| **BM25** <br>（キーワード検索） | 単語頻度と逆文書頻度に基づく確率的ランキング関数。統計的手法でスコアを計算し、正確なキーワードクエリ、製品型番検索、法令検索、文書ランキングなど、精密なマッチングが必要なシーンで優れている。 | ✅**連携の利点**: スコア分布が細かく融合効果が高い、業界標準でエコシステムが成熟、キーワードの精密マッチングが非常に速い。<br>❌ **連携の欠点**: 二重のインデックスシステムが必要、意味理解がなく同義語を扱えない。 | **実装**: 極めて低い（訓練不要）<br>**構築**: 極めて低い<br>**クエリ**: 極めて低い<br>**メモリ**: 低い |
| **TF-IDF** <br>（キーワード検索） | 単語頻度-逆文書頻度の統計的重み計算。文書集合における語彙の重要性を評価し、主に文書分類、情報検索、特徴抽出、基準比較などの基礎的な分析作業に用いられる。 | ✅**連携の利点**: 実装が簡単でリソース消費が少ない、計算が速く広くサポートされている。<br>❌ **連携の欠点**: スコア分布が粗く融合効果が低い、現代のシステムでのサポートが限定的、意味を理解できない。 | **実装**: 極めて低い（古典的アルゴリズム）<br>**構築**: 極めて低い<br>**クエリ**: 極めて低い<br>**メモリ**: 低い |
| **IndexFlatIP (または Flat)**<br>（ベクトル検索-厳密検索） | 100%正確な厳密検索手法。クエリベクトルとデータベース内の全ベクトルとの真の距離を計算し、近似を行わず、総当たりで検索する。小規模データセット（< 5万件）の検証、他のインデックスの正確性の「黄金標準」としての比較、学術研究、プロトタイプ開発に適している。 | ✅**連携の利点**: 正確性の基準として標準的、100%正確な結果で信頼性が高い。<br>❌ **連携の欠点**: 本番環境には適用できず小規模テストにのみ適している、クエリが非常に遅く大規模応用には不向き。 | **実装**: 極めて低い（総当たり検索）<br>**構築**: 極めて低い<br>**クエリ**: 非常に高い<br>**メモリ**: 低い |
| **IVFFlat (Inverted File Flat)**<br>（ベクトル検索-ANN検索） | 「分割統治」のインデックス戦略。逆ファイル構造を採用し、全ベクトルをnlist個のクラスタ/パーティションに分割。クエリ時には、答えを含む可能性が最も高いnprobe個のパーティションのみを検索し、速度とメモリ使用量の良好なバランスを実現する。メモリ制約のある環境、数百万から数億規模のデータ、オフラインバッチ処理、コスト重視の応用に適している。 | ✅**連携の利点**: メモリ効率が高くコスト管理に優れる、大規模データ処理に適している、速度とメモリのバランスが良い。<br>❌ **連携の欠点**: HNSWより精度がやや低い、境界付近のベクトルが欠落し精度に影響する可能性がある。 | **実装**: 中程度（`nlist`、`nprobe`の調整が必要）<br>**構築**: 中程度<br>**クエリ**: 中程度<br>**メモリ**: 低い |
| **HNSW (Hierarchical Navigable Small World)**<br>（ベクトル検索-ANN検索） | 現在、速度と正確性の総合性能で最も優れていると認められているインデックスタイプ。「高速道路網」のような多層グラフ構造を構築し、ベクトル空間内で最も類似した近傍点へ極めて高速にナビゲートできる。リアルタイムQ&Aシステム、推薦システム、大規模オンラインサービス、低遅延要求などのシーンに特に適している。 | ✅**連携の利点**: クエリ速度が非常に速くユーザー体験が良い、精度が高く結果の品質が安定、速度と正確性の総合性能が最も良い。<br>❌ **連携の欠点**: メモリコストが比較的高く、パラメータ調整が複雑で精密な調整が必要。 | **実装**: 高い（`M`、`ef_construction`、`ef_search`の調整が必要）<br>**構築**: 中程度<br>**クエリ**: 極めて低い<br>**メモリ**: 高い |
| **FAISS** <br>（ベクトル検索ライブラリ） | Facebookがオープンソース化した高効率な類似性検索ライブラリ。多様なインデックスタイプとGPUアクセラレーションをサポートし、豊富なインデックス選択肢、高度な最適化、工業レベルの安定性を提供する。主に大規模ベクトル検索、GPUアクセラレーションが必要な場合、多様なインデックス戦略、本番レベルのデプロイなど、エンタープライズ級の応用に用いられる。 | ✅**連携の利点**: 多様なインデックスタイプをサポートし柔軟性が高い、GPUアクセラレーションによる強力な処理能力、豊富なインデックス選択肢と工業レベルの安定性。<br>❌ **連携の欠点**: 学習コストが高く統合が複雑、設定が複雑で依存関係が重い。 | **実装**: 中〜高（異なるインデックスタイプに精通する必要あり）<br>**構築**: 中程度<br>**クエリ**: 低〜中程度<br>**メモリ**: 設定可能 |

---

## 🎯 Cross-encoderによる再ランキング（Reranking）

Cross-encoderによる再ランキングは、RAG検索フローにおける重要な「洗練」ステップであり、検索フローの第二段階である精密ランキング段階に位置づけられる。「リコール」段階の後、候補文書を再検討し、精密に並べ替える。このプロセスは採用面接に似ている。第一段階のリコールが人事部による100人の候補者の書類選考だとすれば、リランキングは技術責任者による深掘り面接で、その中からトップ3〜5人を選び出す作業だ。

Cross-encoder再ランキングが解決しようとする核心的な問題は、従来の検索手法の限界に起因する。我々が検索に用いるエンベディングモデル（バイエンコーダと呼ばれる）は、「クエリ」と「文書」を別々にベクトルにエンコードしてからその類似度を比較する。この方法は高速で、大規模な一次選考に適している。しかし、この「分離比較」方式では、クエリと文書の間の微細で相互作用的な意味的関連性を捉えることができない。Cross-encoderの目的は、より正確だがより遅い方法で、この欠点を補うことにある。

動作原理として、Cross-encoderはまず第一段階のリコールから得られたTop-K個の候補文書（例：K=100）を受け取る。次にペアワイズでの精密計算を行う。Cross-encoderは「クエリ」と「各候補文書」をペアにして、より強力なTransformerモデルに同時に投入する。この専門の機械学習モデルが「審判」として機能し、評価基準は「ユーザーの元のクエリ」と「各候補文書」との間の「意味的関連性」である。各「クエリ-文書」の組み合わせに対して、正確な関連性スコア（通常0から1の間）を出力する。最後にこのスコアに基づいて再ランキングを行い、最もスコアの高いTop-N個を最終的な検索結果として選び出す。

Cross-encoder再ランキングの利点は、その極めて高い精度にあり、検索結果の関連性を向上させるための「黄金標準」と広く認識されている。欠点は、非常に遅く、計算コストが極めて高いことである。各「クエリ-文書ペア」に対して完全なモデル推論を行う必要があり、計算コストは候補集合のサイズに比例し、事前にインデックスを構築することもできない。

### リランカーツール

リランカーツールは主に3つのカテゴリに大別され、それぞれに異なる利点と適用シーンがある。**オープンソースのCross-Encoderモデル**は、クエリと文書をペアでTransformerモデルに投入し、深い相互作用比較を行う。学術界で最も効果的と認められている方法で、精度に対する要求が極めて高く、技術チームによるメンテナンスが可能な場合に適している。**商用リランクAPI**（Cohere Rerankなど）はパッケージ化されたサービスを提供し、ハードウェアやモデルの管理が不要なため、迅速なデプロイや中小規模のアプリケーションに適している。**リランカーとしてのLLM**は最も柔軟な新しいアプローチで、プロンプトエンジニアリングを通じて複雑なランキングロジックや解釈可能性を実現できるが、コストは最も高い。

戦略の選択は優先順位に依存する。究極の精度を追求し技術力があるならオープンソースモデルを、迅速なデプロイとコスト管理が必要なら商用APIを、複雑なロジックと高度なカスタマイズが必要ならLLMを選択する。

| ソリューションタイプ | 代表的なツール | 主な利点 | 主な欠点 | 適用シーン |
|---|---|---|---|---|
| **オープンソースCross-Encoder** | bge-reranker-large <br>sentence-transformers | 最高の精度<br>完全なコントロールとプライバシー保護<br>深い相互作用計算 | GPUリソースが必要<br>デプロイと管理が複雑<br>計算コストが高い | 精度要求が極めて高い<br>技術チームによる保守<br>厳格なプライバシー要件 |
| **商用リランクAPI** | Cohere Rerank <br>Jina AI <br>Voyage AI | 使いやすさ<br>高性能な最適化<br>ハードウェア管理不要 | 従量課金制のコスト<br>データプライバシーリスク<br>サードパーティへの依存 | 迅速なデプロイが必要<br>中小規模アプリケーション<br>コスト管理可能な範囲 |
| **リランカーとしてのLLM** | GPT-4 <br>Claude 3 <br>Llama 3 | 非常に高い柔軟性<br>高い解釈可能性<br>複雑なロジックのサポート | 最高のコスト<br>最長の遅延<br>出力形式が不安定 | 複雑なランキングロジックが必要<br>高度なカスタマイズ要求<br>解釈可能性が重要 |

### ランキング手法への介入と訓練

RAGシステムにドメイン専門家の知識を持たせる鍵は、リランカーのカスタマイズにある。**プロンプトエンジニアリングによる介入**はLLMリランカーに適しており、プロンプトに重み付けの指示やランキングの好みを加えることで、ランキングロジックを迅速に調整できる。**ファインチューニングによる訓練**はオープンソースのCross-Encoderモデルに適しており、人手でラベル付けされたトリプレットデータセットを通じて、汎用モデルに特定ドメインの意味的な差異を学習させる。

*   **プロンプト重み付けの例**: `"内容の関連性に加え、文書の出典が『法務部』であれば、そのスコアの重みを1.2倍にする。文書のタイトルに『最終版』が含まれていれば、そのスコアの重みを1.5倍にする。"`
*   **トリプレットデータの例**: `{ "query": "量子電池の過熱問題について", "positive_passage": "文書Aの段落。電池の放熱解決策を詳述している", "negative_passage": "文書Bの段落。量子電池の利点のみに言及し、過熱問題には触れていない" }`

**推奨戦略**: まずは事前訓練済みのCross-Encoderか商用APIから始める。複雑なロジックが必要になったらLLMリランカーを試し、究極の精度を追求するなら専用モデルのファインチューニングに投資する。

---

## ⚙️ 組み合わせ応用戦略とベストプラクティス

実際の生産環境では、単一の戦略だけを用いることは稀で、複雑な要求に対応するために、まるで積み木を組み立てるかのように、異なる技術を組み合わせることが多い。完全な検索戦略は、検索前の事前フィルタリング、検索時の組み合わせ応用、そして検索後の再ランキングによる洗練という3つの段階から構成される。

### 検索前：事前フィルタリング戦略 (Pre-filtering)

これは**すべての生産レベルのシステムが採用すべき黄金の組み合わせ**である。リソースを消費するANNベクトル検索を実行する前に、まず正確な**メタデータ**でフィルタリングを行う。これにより、クエリの速度と精度が大幅に向上し、同時に計算コストも削減される。リコールされた結果が意味的に関連しているだけでなく、ユーザーの確定的な要求に完全に合致していることを保証する。クエリが来ると、システムはまず`WHERE`句（例：`category = '法律' AND year > 2024`）を実行し、検索範囲を数千万のベクトルから数千に絞り込み、**その後で**その数千のベクトルに対して`HNSW`や`IVFFlat`検索を実行する。

### 検索時：組み合わせの応用 (Hybrid Strategies)

検索段階では、さまざまな要求に応じたパフォーマンスを最適化するために、多様な組み合わせ戦略を採用できる。

#### IVF + HNSW (ハイブリッドインデックス)

一部の先進的なベクトルデータベース（Milvusなど）で採用されている2層のインデックス戦略で、両者の利点を組み合わせている。まずIVFの考え方を用いて、数十億という膨大なベクトルセットを数千のパーティションに大まかに分割する。そして、**各**小さなパーティション内部でHNSWを用いてベクトルを組織化する。これにより、単一のHNSWグラフインデックスが超大規模（数十億レベル）でメモリ爆発を引き起こす問題を解決する。クエリ時には、まず少数のパーティションを高速に特定し、その後それらのパーティション内で極めて高速なHNSW検索を行う。これは**究極のスケール**において、メモリ、速度、精度のバランスを取る効率的な方法である。

#### ハイブリッド検索 (Hybrid Search: vDB + BM25)

純粋なベクトル検索（vDB）は意味を理解できるが、「**キーワード**」には非常に鈍感である。例えば、ユーザーが「`PostgreSQL v15.3`のバグ」と検索した場合、ベクトル検索は「データベースのパフォーマンス問題」や「PostgreSQLの新機能」に関する多くの文書を返すが、タイトルがまさに「Report: Bug in `PostgreSQL v15.3`」である重要な文書を見逃す可能性がある。一方、従来のキーワード検索（`BM25`アルゴリズムが代表）は「`PostgreSQL`」と「`v15.3`」という単語を正確にマッチングできるが、「データベースのパフォーマンス問題」と「database performance issue」が同じ意味であることは全く理解できない。**ハイブリッド検索の目的は、両者の長所を組み合わせること**にある。正確なキーワードを含む「字面上の関連」文書を見逃さず、かつクエリの意味に近い「意味的な関連」文書も逃さない。

ユーザーがクエリを出すと、システムは**同時に**そのクエリを2つの異なる検索エンジンに送信する。一つは**ベクトルデータベース（vDB）**でベクトル類似度検索を行い、もう一つは**BM25ベースの全文検索エンジン**（Elasticsearchや一部のvDBの組み込み機能など）でキーワード検索を行う。システムはそれぞれスコア付きの2つの異なるランキングリストを得る。次に、融合アルゴリズム（最も一般的には**Reciprocal Rank Fusion, RRF**）を用いて、これら2つのリストをより包括的なランキング結果に統合し、第一段階のリコール成果とする。

ハイブリッド検索のリコール率は非常に高く、「網の目から漏れる魚」を大幅に減らし、システムはより堅牢になり、特定のタイプのクエリで簡単には失敗しなくなる。キーワードと意味を両立させることで、ユーザーフレンドリーになり、正確な製品型番を入力しても、曖昧な説明的な質問をしても、良好な初期結果を返すことができる。しかし、アーキテクチャの複雑さは増し、2つのインデックスシステム（ベクトルインデックスとキーワードインデックス）を維持する必要がある。また、2つのソースからのランキング結果をどのように融合させるかという融合戦略自体が、調整を要する技術的なポイントとなる。初期結果にはまだノイズが含まれている可能性がある。なぜなら、広く網をかけるため、リコールされた候補セット（例えばTop 100）の中の関連文書の「純度」は最高ではないかもしれないからだ。

### 検索後：再ランキングによる洗練 (ANN + Re-ranker)

これは**究極の精度**を追求する際に採用される検索戦略である。ベクトル検索の**速度**とリランカーの**精度**を完璧に組み合わせている。これは、現在多くのトップレベルのRAGシステムが回答の質を向上させるための秘密兵器である。

**第一段階 - リコール (Recall)**：非常に高速なANNインデックス（例えば、`HNSW`で`ef_search`を低めに設定）を使用して、膨大なデータの中から比較的大規模な候補セット（例えばTop 50-100件）を迅速にリコールする。この段階の目標は「見逃すよりは、誤って多く拾う方がまし」である。

**第二段階 - 精密ランキング (Precision)**：この100件の候補結果を、より強力だが計算が遅い「**リランカーモデル**」（通常はCross-Encoderモデル）に渡す。このモデルは、ユーザーのクエリと各候補文書の内容を深く比較・採点し、最終的に最も正確なTop 3-5件の結果を選び出す。

**効果**：

### ベストプラクティス：完全な検索アーキテクチャ

SOTA（最先端）のRAGシステムでは、3つの段階を連携させる。事前フィルタリングで検索範囲を絞り込み → ハイブリッド検索でTop 100の候補文書をリコール → Cross-encoderリランカーでTop 3-5に精密ランキング → LLMが回答を生成。この4段階のプロセスは、リコールの網羅性を保証しつつ、最終的なコンテキストの究極の精度を確保する。実際の応用では、HNSWから始め、必ず事前フィルタリングと組み合わせることを推奨する。メモリのボトルネックに遭遇した場合はIVFFlatを、精度のボトルネックにはリランキングを追加し、スケールのボトルネックにはIVF+HNSWのハイブリッドインデックスを使用する。最適なインデックス戦略は、データ分布、クエリパターン、およびビジネス要件の深い理解から生まれる。

## 🚀 リコールの最適化

### 🧠 コンテキスト認識型検索 (Context-aware Retrieval)

コンテキスト認識型検索とは、関連するチャンクを検索した後、それらの散在した断片を直接使用するのではなく、メタデータを利用してチャンクが属する完全な文書またはページ単位に遡り、重複排除と再ランキングを行い、最終的により完全なコンテキスト単位で回答を生成する検索戦略である。

従来のRAGは、チャンキングによって関連コンテンツが異なるチャンクに分割される可能性があり、生成時に完全な文脈が欠如するという問題があった。コンテキスト認識型検索は、「まず断片を見つけ、次にページ全体を取得する」という方法で、LLMが問題を理解し回答するのに十分なコンテキストを確保する。これは、標準的なRAGが「チャンキング」によって引き起こす「**コンテキストの断片化（Context Fragmentation）**」問題を解決するための高度な手法である。

**核心的なフローの転換**：

- **標準RAG**：検索（チャンク）→ ランキング（チャンク）→ 生成
- **コンテキスト認識型検索**：検索（チャンク）→ 拡張（文書/ページ）→ 重複排除 → ランキング（文書/ページ）→ 生成

コンテキスト認識型検索の最大の利点は、**完全なコンテキスト**を提供し、LLMが孤立した断片ではなく完全なページを取得できるため、より一貫性のある回答を生成できることにある。この方法は、特に答えが文書の異なる部分に散在している**複雑な問題**の処理に適している。さらに、Cross-encoderは完全なページに基づいて関連性を判断する際、より多くの文脈的手がかりがあるため、より**正確**になる。

しかし、この戦略には明らかな代償も伴う。まず**遅延の増加**である。システムは追加のコンテンツ取得ステップを必要とし、I/O操作時間が増加する。さらに深刻なのは**コストの大幅な増加**の問題で、LLMが処理するトークン数が8〜10倍に増加し、API費用が倍増する可能性がある。同時に、ページ内のノイズコンテンツがLLMの注意を散漫にさせ、かえって回答の質を低下させる**黄金の段落の希釈リスク**も存在する。最後に、全体の**アーキテクチャが複雑化**し、ベクトルデータベースと文書ストレージの2つのシステムを同時に維持する必要がある。

#### 実装アーキテクチャとコスト最適化

技術アーキテクチャには、明確なメタデータを持つチャンクを保存するベクトルデータベース（Pinecone、pgvectorなど）、元のコンテンツを高速に取得するための文書ストレージ（S3、Redis、または既存のデータベース）、そしてワークフロー全体を編成するオーケストレータ（LangChain、LlamaIndex）が必要である。

**オーケストレータの核心的なタスク**は、効率的なキャッシュ層を構築することである。オーケストレータと文書ストレージの間にRedisなどのキャッシュシステムを追加することで、重複したI/O操作を回避する。同時に、**柔軟なコンテキスト戦略**を実装する。文書の長さに応じてスライディングウィンドウコンテキスト（重要なチャンクの前後各1チャンクを取得）を選択するか、全文の代わりに要約を使用する（高速なLLMでまず要約してから処理する）ことで、コンテキストを提供しつつトークン数を制御する。

しかし、最も重要なコスト管理戦略は**インテリジェントな段階的処理**である。これは適応型RAG戦略であり、システムはまず問題の複雑さを判断する。単純な問題は標準的なチャンクベースのRAGフローを通り、複雑な問題のみが高価なコンテキスト拡張と再ランキングのフローをトリガーする。この段階的処理により、コンテキスト認識型検索はRAGシステムの兵器庫における「攻城砲」として位置づけられ、深い理解が最も必要なクエリにのみ使用されることで、回答の品質とシステムコストの最適なバランスを実現する。

### 🔗 複数データベースの連携

企業の応用シーンでは、しばしば複数のデータソースを統合する必要がある。これには、意味的に大きく異なる複数のベクトルデータベースや、従来のリレーショナルデータベースとベクトルデータベースのハイブリッド利用が含まれる。

#### フェデレーテッドRAG (Federated RAG)

意味的に大きく異なる複数のベクトルデータベース（法律、販売、画像、映像など）から成るフェデレーテッドRAGシステムは、多様な異種データソースを統合して企業レベルのナレッジセンターを形成する必要がある。このような複雑な複数データベース環境において、リコール率と精度を向上させるには、より高次の「インテリジェント検索オーケストレータ」を設計する必要がある。

最大の課題は、分野横断的な洞察を逃さずに（高いリコール率）、最も関連性の高い情報源を迅速に特定し、異なるソースからの結果を正確で一貫した一つの回答に融合させる（高い精度）ことである。

**インテリジェント検索オーケストレータのアーキテクチャ**：賢い図書館司書のように、質問されると、1）まずどのデータベースを探しに行くか判断し、2）同時に複数の場所を検索し、3）見つかった結果をランキング順に統合し、4）最後に統一された基準で再ランキングする。前提として、すべてのデータベースが比較可能であるためには、同じ「言語」（エンベディングモデル）を使用している必要がある。

#### ポリグロット・パーシステンス・アーキテクチャ (Polyglot Persistence)

ベクトルデータベースはすべてのシナリオに適しているわけではない。高度に構造化され、数値が正確で、キーワード検索に依存するデータ（在庫管理、財務報告、ユーザーアカウントなど）は、リレーショナルデータベースやNoSQLの方がパフォーマンスが良く、LLMのトークンを消費する必要もない。核心的な原則は「**多様な永続化（Polyglot Persistence）**」——つまり、異なる種類の作業に最も適したデータベースを選択することである。

検索段階で異なるデータ（意味に適したものはベクトルデータベースに、構造化されたものはリレーショナルデータベースに）をまとめて取得し分析するには、RAGシステムを「検索機」から「**インテリジェント・データ・エージェント**」にアップグレードする必要がある。

**データ・エージェントのアーキテクチャ**：賢いアシスタントのように、あなたの質問を聞くと、まず複雑な問題を小さなタスクに分解し、どのタスクが正確な数値を必要とするか（従来のデータベースを使用）、どのタスクが意味理解を必要とするか（ベクトルデータベースを使用）を識別し、それぞれを実行した後、結果を統合して完全な回答を作成する。

鍵となるのは**問題の分解**である。システムは、「販売量」のような正確な数値の問題はSQLで従来のデータベースを検索すべきであり、「顧客評価」のような意味的な問題はベクトル検索を使用すべきであると判断できなければならない。そして、一つの問題を異なるデータベースに適したサブ問題に賢く分割し、それぞれを並行して処理した後に統一的に統合する。これにより、AIは単なるテキストプロセッサから、様々なデータベースを操作できるデジタルアシスタントへと進化する。

#### データベースの取扱説明書の作成

LLMエージェントに各データベースに何が入っているかを「推測」させるのではなく、能動的かつ明確に教える方が良い。つまり、データベースに明確な「**取扱説明書**」を提供することだ。Agentic RAGアーキテクチャにおいて、この「取扱説明書」とは、「ツール（Tool）」を定義する際に付随する「**説明（Description）**」のことである。

**簡単に言えば、使用マニュアルを書くこと**：LangChainやLlamaIndexなどのエージェントフレームワークを使用する際、各データベースを「ツール（Tool）」として定義し、そのツールの説明欄に、このデータベースが何をするためのものかを明確に記述する。

**従来型データベースのツール説明例**：
「このツールは会社のERPシステムを照会するために使用します。問題が具体的な販売数、在庫レベル、製品価格、または注文状況に関するものである場合、このツールを優先的に使用すべきです。」

**ベクトルデータベースのツール説明例**：

- **法律文書ベクトルストア**：`search_legal_documents` - 契約書、法務メモ、コンプライアンス報告書などを検索
- **販売市場ベクトルストア**：`search_marketing_materials` - 製品ホワイトペーパー、顧客事例、市場分析などを検索
- **画像ベクトルストア**：`search_images` - 製品写真、イベント写真、設計図などを検索

**エージェントの総合プロンプト例**：

```
あなたは万能の企業分析アシスタントです。利用可能なツールリスト：
1. query_erp_database(sql_query: str) - 販売、在庫などの正確な業務データを照会
2. search_legal_documents(query: str) - 法律契約書とコンプライアンス文書を検索
3. search_marketing_materials(query: str) - 製品紹介と市場分析を検索
4. search_images(query: str) - 説明に基づいて画像を検索

タスク指示：ユーザーの質問に基づき、必要な情報を考え、最適なツールを選択して情報を取得し、最後にすべての情報を統合して包括的で正確な回答を提供してください。
```

こうすることで、AIはいつどのデータベースを使うべきかを知ることができ、自分で推測する必要がなくなる。